{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rasp-mibi: Recurrence And Survival Prediction via Multiplexed Ion Beam Imaging\n",
    "\n",
    "*Multiplexed Imaging Analysis of the Tumor-Immune Microenvironment Reveals Predictors of Outcome in Triple-Negative Breast Cancer.*  \n",
    "Aalok Patwa, Rikiya Yamashita, Jin Long, Michael Angelo, Leeat Keren, Daniel Rubin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [Calculating Cell Prevalence](#calc_prevalence)\n",
    "* [Calculating Protein Expression](#calc_expression)\n",
    "* [Immune Composition Analysis](#comp)\n",
    "* [Protein Expression Analysis](#expression)\n",
    "* [Calculating Protein Co-Expression](#calc_coexpr)\n",
    "* [Protein Co-Expression Analysis](#coexpr)\n",
    "* [Voronoi-based Interactions](#voronoi)\n",
    "* [Functional Protein Interactions Analysis](#functional)\n",
    "* [Immunoregulatory Protein Interactions Analysis](#immuno)\n",
    "* [Multivariate Cox Regression Analysis](#multi_cox)\n",
    "* [Multivariate Random Survival Forest Analysis](#rsf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Cell Prevalence & Immune Composition <a name=\"calc_prevalence\"></a>\n",
    "**Purpose**: calculate the proportion of cells of each cell type in each patient's image.  \n",
    "**Output**: a CSV file in the intermediate_data/ folder indicating the prevalence of each cell type in each patient's image.\n",
    "### NOTE\n",
    "This code cannot be run without the raw MIBI data. The data is not included with this repo.  \n",
    "As such, this code is simply to understand the flow of the analysis, not to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np \n",
    "import PIL \n",
    "import pandas as pd\n",
    "from skimage import measure\n",
    "import scipy\n",
    "\n",
    "imagepath = \"rawdata/TNBCcelltypes\"\n",
    "\n",
    "#Keep track of the counts using a matrix of size n_patients X n_cells\n",
    "all_info = []\n",
    "\n",
    "#Iterate through all patients\n",
    "for patient in os.listdir(imagepath):\n",
    "\n",
    "    #Find out the recurrence label of this patient\n",
    "    identifier = int(patient.split(\"_\")[0][1:])\n",
    "    \n",
    "    if identifier == 30 or identifier == 22 or identifier == 38:\n",
    "        continue\n",
    "    \n",
    "    #Read image\n",
    "    image = cv2.imread(os.path.join(imagepath,patient), 0)\n",
    "    \n",
    "    #Connect components\n",
    "    labeled, total_cells = measure.label(image, return_num=True)\n",
    "    \n",
    "    #Find the number of tumor cells in the image (for future reference)\n",
    "    nr_tumor = measure.label((image == 4), return_num=True)[1]\n",
    "    \n",
    "    information = []\n",
    "    information.append(identifier)\n",
    "    \n",
    "    information.append(total_cells)\n",
    "    \n",
    "    #Find the counts of all celltypes\n",
    "    for celltype in range(2,17):\n",
    "        selected_cell = (image == celltype)\n",
    "        labeled, nr = measure.label(selected_cell, return_num=True)\n",
    "        nr /= total_cells\n",
    "        information.append(nr)\n",
    "    \n",
    "    \n",
    "    all_info.append(information)\n",
    "   \n",
    "#Create a DataFrame of the counts with the names of the cells rather than numbers \n",
    "countdf = pd.DataFrame(all_info, columns=[\"ID\", \"Total Cells\", \"Endothelial\", \"Mesenchyme\",\n",
    "                                         \"Tumor\", \"Treg\", \"CD4_T\", \"CD8_T\", \"CD3_T\", \"NK\",\n",
    "                                         \"B\", \"Neutrophil\", \"Macrophage\", \"DC\", \"DC_Mono\",\n",
    "                                         \"Mono_neutrophil\", \"Other\", \"Label\", \"Days\"])\n",
    "\n",
    "countdf.set_index(\"ID\", inplace=True)\n",
    "countdf.sort_index(ascending=True, inplace=True)\n",
    "countdf.to_csv(\"intermediate_data/created_cellprevalence_df.csv\")\n",
    "\n",
    "display(countdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Protein Expression <a name=\"calc_expression\"></a>\n",
    "\n",
    "**Purpose**: Calculate protein expression in each cell of each patient's image and assign positivity to each cell based on a threshold.  \n",
    "**Output**: CSVs of expression levels in the intermediate_data/protein_expression/ folder and positivity assignments in intermediate_data/created_protein_positivity/  \n",
    "\n",
    "### NOTE\n",
    "This code cannot be run without the raw MIBI data. The data is not included with this repo.  \n",
    "As such, this code is simply to understand the flow of the analysis, not to be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import Delaunay, delaunay_plot_2d, Voronoi, voronoi_plot_2d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import seaborn\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "celltype_dir = \"rawdata/TNBCcelltypes/\"\n",
    "marker_dir = \"rawdata/MIBITIFF/\"\n",
    "marker_frames = \"rawdata/proteins_by_frame.csv\"\n",
    "savedir = \"intermediate_data/protein_expression/\"\n",
    "\n",
    "frames = pd.read_csv(marker_frames)\n",
    "\n",
    "columns = frames.index.values.tolist()\n",
    "columns.insert(0, \"Celltype\")\n",
    "columns.insert(0, \"Column\")\n",
    "columns.insert(0, \"Row\")\n",
    "\n",
    "\n",
    "for patient in os.listdir(celltype_dir):\n",
    "    identifier = patient.split(\"_\")[0][1:]\n",
    "    \n",
    "    marker_path = marker_dir + \"p\" + identifier + \".tif\"\n",
    "    combined_im = create_multimarker_image(marker_path)\n",
    "    cv2_im = cv2.imread(os.path.join(celltype_dir, patient), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    labeled, nr_objects = skimage.measure.label(cv2_im, return_num=True)\n",
    "    props = skimage.measure.regionprops(labeled)\n",
    "    \n",
    "    all_cells = []\n",
    "    for cell in range(nr_objects):\n",
    "        centroid_x = int(props[cell].centroid[0])\n",
    "        centroid_y = int(props[cell].centroid[1])\n",
    "        \n",
    "        celltype = cv2_im[centroid_x, centroid_y]\n",
    "\n",
    "        binary_mask = (labeled == cell)\n",
    "        biom_expression = combined_im[binary_mask]\n",
    "        size = biom_expression.shape[0]\n",
    "        expression_vector = np.sum(biom_expression, axis=0) / size\n",
    "        expression_vector = expression_vector.tolist()\n",
    "        expression_vector.insert(0, celltype)\n",
    "        expression_vector.insert(0, centroid_y)\n",
    "        expression_vector.insert(0, centroid_x)\n",
    "        all_cells.append(expression_vector)\n",
    "    \n",
    "    dataframe = pd.DataFrame(all_cells, columns=columns)\n",
    "    dataframe.to_csv(savedir + identifier + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign cells as \"positive\" or negative for each protein based on their expression level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the expression levels in the background (interstitial space)\n",
    "celltype_dir = \"rawdata/TNBCcelltypes/\"\n",
    "marker_dir = \"rawdata/MIBITIFF/\"\n",
    "\n",
    "expression_vector = np.zeros(44)\n",
    "total_pixels = 0\n",
    "\n",
    "for patient in os.listdir(celltype_dir):\n",
    "    identifier = patient.split(\"_\")[0][1:]\n",
    "    if identifier == \"30\" or identifier == \"22\" or identifier == \"38\":\n",
    "        continue\n",
    "    \n",
    "    marker_path = marker_dir + \"p\" + identifier + \".tif\"\n",
    "    combined_im = create_multimarker_image(marker_path)\n",
    "    cv2_im = cv2.imread(os.path.join(celltype_dir, patient), cv2.IMREAD_GRAYSCALE)\n",
    "    dim = cv2_im.shape[0]\n",
    "\n",
    "    #The TIFFs have a 29-pixel black border surrounding the image which is not a part of the sample. Exclude it.\n",
    "    cells_only = cv2_im[29:dim-29, 29:dim-29]\n",
    "    background_mask = (cells_only == 0)\n",
    "    combined_im = combined_im[29:dim-29, 29:dim-29]\n",
    "    \n",
    "    #Extract just one cell\n",
    "    selection = combined_im[background_mask]\n",
    "    size = selection.shape[0]\n",
    "    total_pixels += size\n",
    "    expression_vector += np.sum(biom_expression, axis=0)\n",
    "\n",
    "final_vector = expression_vector / total_pixels\n",
    "\n",
    "\n",
    "\n",
    "#Binarize the matrices according to the threshold calculated across all the images.\n",
    "\n",
    "expressiondir = \"intermediate_data/protein_expression/\"\n",
    "savedir = \"intermediate_data/created_protein_positivity/\"\n",
    "\n",
    "threshold = final_vector\n",
    "for patient in os.listdir(expressiondir):\n",
    "    patient_glcm = pd.read_csv(expressiondir + patient, index_col=\"Unnamed: 0\")\n",
    "    for column in range(len(patient_glcm.columns[3:])):\n",
    "        col_threshold = threshold[column]\n",
    "        patient_glcm[patient_glcm.columns[3+column]] = (patient_glcm[patient_glcm.columns[3+column]] > col_threshold).astype(\"int32\")\n",
    "    patient_glcm.to_csv(savedir + patient, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immune Composition <a name=\"comp\"></a>\n",
    "**Purpose**: determine whether immune composition is associated with recurrence or survival. \n",
    "**Ouput**: two results CSVs in results/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from lifelines import CoxPHFitter\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "\n",
    "cellprevalence_path = \"intermediate_data/cellprevalence_df.csv\"\n",
    "\n",
    "cellprevalence_df = pd.read_csv(cellprevalence_path, index_col=\"ID\")\n",
    "\n",
    "#Get a list of all the cell types to test\n",
    "each_celltype = cellprevalence_df.columns[:15]\n",
    "\n",
    "univariate_results = []\n",
    "\n",
    "for celltype in each_celltype:\n",
    "    \n",
    "    this_cell = cellprevalence_df[[celltype, \"Recurrence_time\", \"Recurrence\"]]\n",
    "    #Perform univariate Cox regression for this cell type\n",
    "    cox = CoxPHFitter()\n",
    "    cox.fit(this_cell, duration_col=\"Recurrence_time\", event_col=\"Recurrence\")\n",
    "    summary_df = cox.summary\n",
    "    coefficients = summary_df[\"coef\"].values[0]\n",
    "    hazards = cox.hazard_ratios_[0]\n",
    "    p_values = summary_df[\"p\"].values[0]\n",
    "    \n",
    "    univariate_results.append([celltype, coefficients, hazards, p_values])\n",
    "\n",
    "univariate_results_df = pd.DataFrame(univariate_results, columns=[\"Celltype\", \"Coef\", \"HR\", \"P\"])\n",
    "univariate_results_df.sort_values(by=\"P\", ascending=True, inplace=True)\n",
    "\n",
    "p_values = univariate_results_df[\"P\"]\n",
    "\n",
    "corrected = multipletests(p_values, method=\"fdr_bh\")[1]\n",
    "\n",
    "univariate_results_df[\"BH-Corrected FDR\"] = corrected\n",
    "\n",
    "univariate_results_df.to_csv(\"results/immune_composition_recurrence.csv\", index=False)\n",
    "display(univariate_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellprevalence_path = \"intermediate_data/cellprevalence_df.csv\"\n",
    "\n",
    "cellprevalence_df = pd.read_csv(cellprevalence_path, index_col=\"ID\")\n",
    "\n",
    "#Get a list of all the cell types to test\n",
    "each_celltype = cellprevalence_df.columns[:15]\n",
    "\n",
    "univariate_results = []\n",
    "\n",
    "for celltype in each_celltype:\n",
    "    \n",
    "    this_cell = cellprevalence_df[[celltype, \"Survival_time\", \"Survival\"]]\n",
    "    #Perform univariate Cox regression for this cell type\n",
    "    cox = CoxPHFitter()\n",
    "    cox.fit(this_cell, duration_col=\"Survival_time\", event_col=\"Survival\")\n",
    "    summary_df = cox.summary\n",
    "    coefficients = summary_df[\"coef\"].values[0]\n",
    "    hazards = cox.hazard_ratios_[0]\n",
    "    p_values = summary_df[\"p\"].values[0]\n",
    "    \n",
    "    univariate_results.append([celltype, coefficients, hazards, p_values])\n",
    "\n",
    "univariate_results_df = pd.DataFrame(univariate_results, columns=[\"Celltype\", \"Coef\", \"HR\", \"P\"])\n",
    "univariate_results_df.sort_values(by=\"P\", ascending=True, inplace=True)\n",
    "\n",
    "p_values = univariate_results_df[\"P\"]\n",
    "\n",
    "corrected = multipletests(p_values, method=\"fdr_bh\")[1]\n",
    "\n",
    "univariate_results_df[\"BH-Corrected FDR\"] = corrected\n",
    "\n",
    "univariate_results_df.to_csv(\"results/immune_composition_survival.csv\", index=False)\n",
    "\n",
    "display(univariate_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Expression <a name=\"expression\"></a>\n",
    "**Purpose**: determine whether the expression of functional proteins is associated with recurrence or survival.  \n",
    "**Output**: two results CSVs in results/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from lifelines import CoxPHFitter\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "expression_path = \"intermediate_data/proteinexpression_df.csv\"\n",
    "\n",
    "proteinexpression_df = pd.read_csv(expression_path, index_col=\"ID\")\n",
    "\n",
    "#indices of the proteins considered to be \"functional\" proteins, and the clinical information\n",
    "functional_protein_indices = [2,6,14,16,22,23,24,25,26,27,28,29,30,31,35,37,38,39, 44, 45, 46, 47]\n",
    "\n",
    "#slice the functional proteins only\n",
    "functional_df = proteinexpression_df.iloc[:, functional_protein_indices]\n",
    "\n",
    "#Leave out the clinical data to iterate over the proteins\n",
    "proteins = functional_df.columns[:-4]\n",
    "\n",
    "univariate_results = []\n",
    "\n",
    "for protein in proteins:\n",
    "    this_protein = functional_df[[protein, \"Recurrence_time\", \"Recurrence\"]]\n",
    "    #Fit the Cox PH model and extract information\n",
    "    cox = CoxPHFitter()\n",
    "    cox.fit(this_protein, duration_col=\"Recurrence_time\", event_col=\"Recurrence\")\n",
    "    summary_df = cox.summary\n",
    "    coefficients = summary_df[\"coef\"].values[0]\n",
    "    hazards = cox.hazard_ratios_[0]\n",
    "    p_values = summary_df[\"p\"].values[0]\n",
    "    \n",
    "    univariate_results.append([protein, coefficients, hazards, p_values])\n",
    "\n",
    "univariate_results_df = pd.DataFrame(univariate_results, columns=[\"Protein\", \"Coef\", \"HR\", \"P\"])\n",
    "univariate_results_df.sort_values(by=\"P\", ascending=True, inplace=True)\n",
    "\n",
    "p_values = univariate_results_df[\"P\"]\n",
    "\n",
    "corrected = multipletests(p_values, method=\"fdr_bh\")[1]\n",
    "\n",
    "univariate_results_df[\"BH-Corrected FDR\"] = corrected\n",
    "\n",
    "univariate_results_df.to_csv(\"results/protein_expression_recurrence.csv\", index=False)\n",
    "\n",
    "display(univariate_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_results = []\n",
    "\n",
    "for protein in proteins:\n",
    "    this_protein = functional_df[[protein, \"Survival_time\", \"Survival\"]]\n",
    "    #Fit the Cox PH model and extract information\n",
    "    cox = CoxPHFitter()\n",
    "    cox.fit(this_protein, duration_col=\"Survival_time\", event_col=\"Survival\")\n",
    "    summary_df = cox.summary\n",
    "    coefficients = summary_df[\"coef\"].values[0]\n",
    "    hazards = cox.hazard_ratios_[0]\n",
    "    p_values = summary_df[\"p\"].values[0]\n",
    "    \n",
    "    univariate_results.append([protein, coefficients, hazards, p_values])\n",
    "\n",
    "univariate_results_df = pd.DataFrame(univariate_results, columns=[\"Protein\", \"Coef\", \"HR\", \"P\"])\n",
    "univariate_results_df.sort_values(by=\"P\", ascending=True, inplace=True)\n",
    "\n",
    "p_values = univariate_results_df[\"P\"]\n",
    "\n",
    "corrected = multipletests(p_values, method=\"fdr_bh\")[1]\n",
    "\n",
    "univariate_results_df[\"BH-Corrected FDR\"] = corrected\n",
    "\n",
    "univariate_results_df.to_csv(\"results/protein_expression_survival.csv\", index=False)\n",
    "\n",
    "display(univariate_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Protein Co-Expression <a name=\"calc_coexpr\"></a>\n",
    "\n",
    "**Purpose**: Calculate instances of co-expression between proteins.  \n",
    "**Output**: intermediate_data/created_coexpression_matrices/. The reader can compare this output to coexpression_matrices/ to ensure reproducibility.  \n",
    "**Estimated time**: 10 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "import seaborn\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "\n",
    "binary_infopath = \"intermediate_data/protein_positivity/\"\n",
    "biomarker_frames = pd.read_csv(\"rawdata/proteins_by_frame.csv\")\n",
    "\n",
    "biom_columns = biomarker_frames[\"Biomarker\"].values\n",
    "\n",
    "for patient in os.listdir(binary_infopath):\n",
    "    print (patient)\n",
    "    com = np.zeros((44,44))\n",
    "    \n",
    "    infodf = pd.read_csv(binary_infopath + patient)\n",
    "        \n",
    "    n_cells = len(infodf.index)\n",
    "    \n",
    "    for cell in range(n_cells):\n",
    "        this_cell = infodf.iloc[[cell]]\n",
    "        pos_columns = []\n",
    "        for column in this_cell.columns[3:]:\n",
    "            if this_cell[column].values[0] == 1:\n",
    "                pos_columns.append(int(column))\n",
    "        combs = combinations(pos_columns, 2)\n",
    "        for combination in combs:\n",
    "            com[combination[0], combination[1]] += 1\n",
    "            com[combination[1], combination[0]] += 1\n",
    "    com_df = pd.DataFrame(com, columns=biom_columns)\n",
    "    com_df.set_index(pd.Index(biom_columns), inplace=True)\n",
    "    com_df.to_csv(\"intermediate_data/created_coexpression_matrices/\" + patient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein Co-Expression <a name=\"coexpr\"></a>\n",
    "\n",
    "**Purpose**: determine whether protein co-expression patterns are predictors of recurrence and survival.  \n",
    "**Output**: two KM curves with log-rank test p-value in results/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "import seaborn\n",
    "\n",
    "\n",
    "#Path to the clinical data about recurrence. \n",
    "clinical_path = \"rawdata/clinical_data.csv\"\n",
    "clinical_df = pd.read_csv(clinical_path, index_col=[\"ID\"])\n",
    "\n",
    "#Path to interaction matrices\n",
    "matrices_path = \"intermediate_data/coexpression_matrices/\"\n",
    "\n",
    "# Out of all proteins, only include the functional proteins\n",
    "markers_to_include = [2,6,14,16,22,23,24,25,26,27,28,29,30,31,35,37,38,39]\n",
    "\n",
    "columns = []\n",
    "feature_list = []\n",
    "\n",
    "#Iterate over all of the co-occurrence matrices in this certain radius. \n",
    "for patient_index in range(len(os.listdir(matrices_path))):\n",
    "    patient_glcm = os.listdir(matrices_path)[patient_index]\n",
    "\n",
    "    #Skip over the pesky .DS_Store file that shows up in Mac file systems.\n",
    "    if (patient_glcm[0] == \".\"):\n",
    "        continue\n",
    "\n",
    "    #Find the internal_ID of the current patient. \n",
    "    identifier = patient_glcm.split(\".\")[0]\n",
    "\n",
    "    #Read the co-occurrence matrix of the current patient and cast to a numpy array excluding the first col.\n",
    "    current_patient_glcm = pd.read_csv(os.path.join(matrices_path, patient_glcm), index_col=\"Unnamed: 0\")\n",
    "    current_patient_glcm.set_index(current_patient_glcm.columns, inplace=True)\n",
    "\n",
    "    #feature_name = current_patient_glcm.columns[chosen_feature]\n",
    "    np_glcm = current_patient_glcm.to_numpy()    \n",
    "\n",
    "    patient_features = []\n",
    "\n",
    "    #Flatten the co-occurrence matrix into a feature vector.\n",
    "    #Cannot simply run np.flatten because the matrix is symmetrical - I only want the top-right triangle\n",
    "    #But, we don't want to include the diagonal\n",
    "    for row in range(np_glcm.shape[0]):\n",
    "        for column in range(row+1, np_glcm.shape[1]):\n",
    "            if row in markers_to_include and column in markers_to_include:\n",
    "                if patient_index == 0:\n",
    "                    feature_name = current_patient_glcm.index[row] + \" \" + current_patient_glcm.columns[column]\n",
    "                    columns.append(feature_name)\n",
    "                patient_features.append(np_glcm[row][column])\n",
    "\n",
    "    #Find the recurrence outcome of this current patient and how long it took for them to recur\n",
    "    #Add these values to the feature list for eventual use in the Kaplan-Meier plot.\n",
    "    try:\n",
    "        patient_features.append(clinical_df.at[int(identifier), \"Recurrence\"])\n",
    "    except KeyError:\n",
    "        continue\n",
    "    patient_features.append(clinical_df.at[int(identifier), \"Recurrence_time\"])\n",
    "    patient_features.append(int(identifier))\n",
    "    feature_list.append(patient_features)\n",
    "\n",
    "#Determine the names of the columns in the DataFrame for easier future access.\n",
    "columns.append(\"Recurrence\")\n",
    "columns.append(\"Recurrence_time\")\n",
    "columns.append(\"ID\")\n",
    "\n",
    "#Create a dataframe using the features, the recurrence events, and the time taken to recur.\n",
    "\n",
    "features_df = pd.DataFrame(feature_list, columns=columns)\n",
    "\n",
    "#Obtain a versino of this dataframe with only the features.\n",
    "data_only = features_df.drop(columns=[\"Recurrence_time\", \"Recurrence\", \"ID\"])\n",
    "data_only[\"Duplicate\"] = features_df[feature_name]\n",
    "data_only.set_index(features_df[\"ID\"], inplace=True)\n",
    "\n",
    "#Create the dendrogram.\n",
    "clustergram = seaborn.clustermap(data_only, method=\"weighted\",\n",
    "                            metric=\"braycurtis\", standard_scale = 1, cmap=\"viridis\", figsize=(10,8), cbar_pos=None)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "#Number of clusters to take from the dendrogram\n",
    "k = 2\n",
    "\n",
    "#Use scipy fcluster to find clusters from the clustered dendrograms.\n",
    "clusters = list(fcluster(clustergram.dendrogram_row.linkage, k, criterion='maxclust'))\n",
    "\n",
    "unique_clusters = len(np.unique(np.array(clusters)))\n",
    "\n",
    "if (unique_clusters < 2):\n",
    "    print (\"Not able to find more than one cluster. \") #If there was no clustering, this method failed.\n",
    "\n",
    "#Create a new column in the DataFrame that includes what cluster each patient falls into\n",
    "features_df[\"clust\"] = clusters\n",
    "\n",
    "first_cluster_count = clusters.count(1)\n",
    "second_cluster_count = clusters.count(2)\n",
    "\n",
    "#Define the KaplanMeierFitter\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "#T = Time, E=Event. These are the two parameters that go into Kaplan-Meier curves. \n",
    "T = features_df[\"Recurrence_time\"]\n",
    "E = features_df[\"Recurrence\"]\n",
    "\n",
    "group1 = (features_df[\"clust\"] == 1)\n",
    "group2 = (features_df[\"clust\"] == 2)\n",
    "\n",
    "T1 = T[group1]\n",
    "E1 = E[group1]\n",
    "T2 = T[group2]\n",
    "E2 = E[group2]\n",
    "\n",
    "color_clust1 = \"#F39B7FFF\"\n",
    "color_clust2 = \"#4DBBD5FF\"\n",
    "\n",
    "# Just for visualization purposes, make the worse-outcome cluster orange and the other blue\n",
    "if E1.mean() < E2.mean():\n",
    "    T1 = T[group2]\n",
    "    E1 = E[group2]\n",
    "    T2 = T[group1]\n",
    "    E2 = E[group1]\n",
    "\n",
    "    features_df[\"clust\"][group1] = 2\n",
    "    features_df[\"clust\"][group2] = 1\n",
    "\n",
    "first_cluster_count = len(T1.index)\n",
    "second_cluster_count = len(T2.index)\n",
    "\n",
    "results_first = logrank_test(T1,T2, event_observed_A=E1, event_observed_B=E2)\n",
    "\n",
    "p1 = round(results_first.p_value, 4)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "kmf.fit(T1, E1, label='Cluster 1: n=' + str(first_cluster_count))\n",
    "ax = kmf.plot(ci_show=False, show_censors=True, color=color_clust1, lw=5)\n",
    "\n",
    "kmf.fit(T2, E2, label='Cluster 2: n=' + str(second_cluster_count))\n",
    "ax = kmf.plot(ax=ax, ci_show=False, show_censors=True, color=color_clust2, lw=5)\n",
    "plt.annotate(\"Log-rank p: \" + str(p1), xy=(0.6, 0.15), xycoords=\"figure fraction\", fontsize=25,\n",
    "        bbox=dict(facecolor='none', edgecolor='black', alpha=0.3, boxstyle=\"Round, pad=0.5, rounding_size=0.2\"))\n",
    "plt.legend(fontsize=25)\n",
    "plt.xlabel(\"Time (days)\", fontsize=25)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Proportion Alive\", fontsize=25)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim(0,1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"results/coexpression_km_recurrence.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the clinical data. \n",
    "clinical_path = \"rawdata/clinical_data.csv\"\n",
    "clinical_df = pd.read_csv(clinical_path, index_col=[\"ID\"])\n",
    "\n",
    "#Path to interaction matrices\n",
    "matrices_path = \"intermediate_data/coexpression_matrices/\"\n",
    "\n",
    "# Out of all proteins, only include the functional proteins\n",
    "markers_to_include = [2,6,14,16,22,23,24,25,26,27,28,29,30,31,35,37,38,39]\n",
    "\n",
    "columns = []\n",
    "feature_list = []\n",
    "\n",
    "#Iterate over all of the co-occurrence matrices in this certain radius. \n",
    "for patient_index in range(len(os.listdir(matrices_path))):\n",
    "    patient_glcm = os.listdir(matrices_path)[patient_index]\n",
    "\n",
    "    #Skip over the pesky .DS_Store file that shows up in Mac file systems.\n",
    "    if (patient_glcm[0] == \".\"):\n",
    "        continue\n",
    "\n",
    "    #Find the internal_ID of the current patient. \n",
    "    identifier = patient_glcm.split(\".\")[0]\n",
    "\n",
    "    #Read the co-occurrence matrix of the current patient and cast to a numpy array excluding the first col.\n",
    "    current_patient_glcm = pd.read_csv(os.path.join(matrices_path, patient_glcm), index_col=\"Unnamed: 0\")\n",
    "    current_patient_glcm.set_index(current_patient_glcm.columns, inplace=True)\n",
    "\n",
    "    #feature_name = current_patient_glcm.columns[chosen_feature]\n",
    "    np_glcm = current_patient_glcm.to_numpy()    \n",
    "\n",
    "    patient_features = []\n",
    "\n",
    "    #Flatten the co-occurrence matrix into a feature vector.\n",
    "    #Cannot simply run np.flatten because the matrix is symmetrical - I only want the top-right triangle\n",
    "    #But, have to include the diagonal as well.\n",
    "    for row in range(np_glcm.shape[0]):\n",
    "        for column in range(row+1, np_glcm.shape[1]):\n",
    "            if row in markers_to_include and column in markers_to_include:\n",
    "                if patient_index == 0:\n",
    "                    feature_name = current_patient_glcm.index[row] + \" \" + current_patient_glcm.columns[column]\n",
    "                    columns.append(feature_name)\n",
    "                patient_features.append(np_glcm[row][column])\n",
    "\n",
    "    #Find the survival outcome of this current patient and how long it took for them to recur\n",
    "    #Add these values to the feature list for eventual use in the Kaplan-Meier plot.\n",
    "    try:\n",
    "        patient_features.append(clinical_df.at[int(identifier), \"Survival\"])\n",
    "    except KeyError:\n",
    "        continue\n",
    "    patient_features.append(clinical_df.at[int(identifier), \"Survival_time\"])\n",
    "    patient_features.append(int(identifier))\n",
    "    feature_list.append(patient_features)\n",
    "\n",
    "#Determine the names of the columns in the DataFrame for easier future access.\n",
    "columns.append(\"Survival\")\n",
    "columns.append(\"Survival_time\")\n",
    "columns.append(\"ID\")\n",
    "\n",
    "#Create a dataframe using the features, the survival events, and the time taken to recur.\n",
    "\n",
    "features_df = pd.DataFrame(feature_list, columns=columns)\n",
    "\n",
    "#Obtain a versino of this dataframe with only the features.\n",
    "data_only = features_df.drop(columns=[\"Survival\", \"Survival_time\", \"ID\"])\n",
    "data_only[\"Duplicate\"] = features_df[feature_name]\n",
    "data_only.set_index(features_df[\"ID\"], inplace=True)\n",
    "\n",
    "#Create the dendrogram.\n",
    "clustergram = seaborn.clustermap(data_only, method=\"weighted\",\n",
    "                            metric=\"cosine\", standard_scale = 1, cmap=\"viridis\", figsize=(10,8), cbar_pos=None)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "#Number of clusters to take from the dendrogram\n",
    "k = 2\n",
    "\n",
    "#Use scipy fcluster to find clusters from the clustered dendrograms.\n",
    "clusters = list(fcluster(clustergram.dendrogram_row.linkage, k, criterion='maxclust'))\n",
    "\n",
    "unique_clusters = len(np.unique(np.array(clusters)))\n",
    "\n",
    "if (unique_clusters < 2):\n",
    "    print (\"Not able to find more than one cluster. \") #If there was no clustering, this method failed.\n",
    "\n",
    "#Create a new column in the DataFrame that includes what cluster each patient falls into\n",
    "features_df[\"clust\"] = clusters\n",
    "\n",
    "first_cluster_count = clusters.count(1)\n",
    "second_cluster_count = clusters.count(2)\n",
    "\n",
    "#Define the KaplanMeierFitter\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "#T = Time, E=Event. These are the two parameters that go into Kaplan-Meier curves. \n",
    "T = features_df[\"Survival_time\"]\n",
    "E = features_df[\"Survival\"]\n",
    "\n",
    "group1 = (features_df[\"clust\"] == 1)\n",
    "group2 = (features_df[\"clust\"] == 2)\n",
    "\n",
    "T1 = T[group1]\n",
    "E1 = E[group1]\n",
    "T2 = T[group2]\n",
    "E2 = E[group2]\n",
    "\n",
    "color_clust1 = \"#F39B7FFF\"\n",
    "color_clust2 = \"#4DBBD5FF\"\n",
    "\n",
    "# Just for visualization purposes, make the worse-outcome cluster orange and the other blue\n",
    "if E1.mean() < E2.mean():\n",
    "    T1 = T[group2]\n",
    "    E1 = E[group2]\n",
    "    T2 = T[group1]\n",
    "    E2 = E[group1]\n",
    "\n",
    "features_df[\"clust\"][group1] = 2\n",
    "features_df[\"clust\"][group2] = 1\n",
    "\n",
    "first_cluster_count = len(T1.index)\n",
    "second_cluster_count = len(T2.index)\n",
    "\n",
    "results_first = logrank_test(T1,T2, event_observed_A=E1, event_observed_B=E2)\n",
    "\n",
    "p1 = round(results_first.p_value, 4)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "kmf.fit(T1, E1, label='Cluster 1: n=' + str(first_cluster_count))\n",
    "ax = kmf.plot(ci_show=False, show_censors=True, color=color_clust1, lw=5)\n",
    "\n",
    "kmf.fit(T2, E2, label='Cluster 2: n=' + str(second_cluster_count))\n",
    "ax = kmf.plot(ax=ax, ci_show=False, show_censors=True, color=color_clust2, lw=5)\n",
    "plt.annotate(\"Log-rank p: \" + str(p1), xy=(0.6, 0.15), xycoords=\"figure fraction\", fontsize=25,\n",
    "        bbox=dict(facecolor='none', edgecolor='black', alpha=0.3, boxstyle=\"Round, pad=0.5, rounding_size=0.2\"))\n",
    "plt.legend(fontsize=25)\n",
    "plt.xlabel(\"Time (days)\", fontsize=25)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Proportion Alive\", fontsize=25)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim(0,1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"results/coexpression_km_survival.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voronoi-based Cell-To-Cell Interactions <a name=\"voronoi\"></a>\n",
    "\n",
    "**Purpose**: calculate cell-to-cell interactions using Voronoi diagrams.  \n",
    "**Output**: interaction matrices in intermediate_data/created_interaction_matrices. The reader can compare this output to interaction_matrices/ to ensure reproducibility.  \n",
    "**Estimated time**: 40 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import Delaunay, delaunay_plot_2d, Voronoi, voronoi_plot_2d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "import seaborn\n",
    "from itertools import combinations\n",
    "from itertools import product\n",
    "\n",
    "def create_voronoi(centdf): #returns the Voronoi diagram object\n",
    "    vor = Voronoi(np.c_[centdf.column.values, centdf.row.values])\n",
    "    return vor\n",
    "def plot_voronoi(vor, identifier): #Void - just plots the graph\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "    fig = voronoi_plot_2d(vor, ax, show_vertices=False, line_colors='blue', \n",
    "                      line_width=2, point_size=2)\n",
    "    ax.set_xlim([0, 2048])\n",
    "    ax.set_ylim([2048, 0])\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/Users/aalokpatwa/Desktop/MIBI/voronoi/voronoi_plotsv2/\" + identifier + \".png\", dpi=300)\n",
    "    plt.close()\n",
    "    #plt.show()\n",
    "\n",
    "def make_voronoi_dataframe(voronoi, centroiddf):\n",
    "    count_skipped = 0\n",
    "    number_centroids = voronoi.points.shape[0]\n",
    "    information_list = []\n",
    "    for centroid in range(number_centroids):\n",
    "        centroid_list = []\n",
    "        centroid_y = voronoi.points[centroid][0]\n",
    "        centroid_x = voronoi.points[centroid][1]\n",
    "        centroid_list.append(centroid)\n",
    "        centroid_list.append((centroid_x, centroid_y))\n",
    "        region_index = voronoi.point_region[centroid]\n",
    "        centroid_list.append(region_index)\n",
    "        vertices = voronoi.regions[region_index]\n",
    "        if -1 in vertices:\n",
    "            count_skipped += 1\n",
    "            continue\n",
    "        else:\n",
    "            vertex_list = []\n",
    "            for vertex in vertices:\n",
    "                y_coord = int(round(voronoi.vertices[vertex][0]))\n",
    "                x_coord = int(round(voronoi.vertices[vertex][1]))\n",
    "                vertex_list.append((y_coord, x_coord))\n",
    "            centroid_list.append(vertex_list)\n",
    "        centroid_list.append(centroiddf.at[centroid, \"celltype\"])\n",
    "        information_list.append(centroid_list)       \n",
    "    infodf = pd.DataFrame(information_list, columns=[\"CentroidIndex\",\"CentroidCoord\", \"RegionIndex\",\n",
    "                                                     \"Vertices\", \"Celltype\"])\n",
    "    return infodf, count_skipped\n",
    "\n",
    "def extract_current_mask(vertexlist):\n",
    "    img = Image.new(\"L\", (2048,2048), 0)\n",
    "    ImageDraw.Draw(img).polygon(vertexlist, fill=1)\n",
    "    mask = np.array(img)\n",
    "    binary_mask = np.where(mask==1)\n",
    "    coordinates = list(zip(binary_mask[0], binary_mask[1]))\n",
    "    return coordinates, len(coordinates)\n",
    "\n",
    "def expression_within_cell(biomarker_image, pixellist, size):\n",
    "    total_vector = np.zeros(44)\n",
    "    for pixel in pixellist:\n",
    "        expression_vector = biomarker_image[pixel[0], pixel[1]]\n",
    "        total_vector += expression_vector\n",
    "    total_vector = total_vector / size\n",
    "    return total_vector\n",
    "\n",
    "def create_multimarker_image(imagepath):\n",
    "    biomarker_pil = Image.open(imagepath)\n",
    "    n_frames = biomarker_pil.n_frames\n",
    "    \n",
    "    first_level = np.array(biomarker_pil, dtype=\"uint8\")\n",
    "    first_level = np.reshape(first_level, (2048,2048,1))\n",
    "        \n",
    "    combined_image = first_level\n",
    "    \n",
    "    for frame in range(1, n_frames):\n",
    "        biomarker_pil.seek(frame)\n",
    "        current_level = np.array(biomarker_pil, dtype=\"uint8\").reshape((2048,2048,1))\n",
    "        combined_image = np.concatenate((combined_image, current_level), axis=2)\n",
    "    return combined_image\n",
    "\n",
    "def create_neighbor_matrix(voronoi, voronoi_df):\n",
    "    number_regions = len(voronoi_df.index)\n",
    "    adjacency_list = []\n",
    "    for region in range(number_regions):\n",
    "        adjacency_list.append([])\n",
    "    ridge_points = voronoi.ridge_points\n",
    "    for edge in ridge_points:\n",
    "        first_centroid = edge[0]\n",
    "        second_centroid = edge[1]\n",
    "        first_cell = voronoi_df[voronoi_df[\"CentroidIndex\"] == first_centroid]\n",
    "        second_cell = voronoi_df[voronoi_df[\"CentroidIndex\"] == second_centroid]\n",
    "        if (first_cell.empty or second_cell.empty):\n",
    "            continue\n",
    "        first_index = int(first_cell.index[0])\n",
    "        second_index = int(second_cell.index[0])\n",
    "        adjacency_list[first_index].append(second_centroid)\n",
    "        adjacency_list[second_index].append(first_centroid)\n",
    "    new_df = voronoi_df.copy()\n",
    "    new_df[\"Adjacency\"] = adjacency_list\n",
    "    return new_df\n",
    "\n",
    "\n",
    "#-------------------------------------------------------\n",
    "# Calculate interactions of each type per image\n",
    "\n",
    "binary_infopath = \"intermediate_data/protein_positivity/\"\n",
    "centroid_path = \"intermediate_data/centroids/\"\n",
    "biomarker_frames = pd.read_csv(\"rawdata/proteins_by_frame.csv\")\n",
    "\n",
    "biom_columns = biomarker_frames[\"Biomarker\"].values\n",
    "\n",
    "for patient in os.listdir(binary_infopath):\n",
    "    print (patient)\n",
    "    if patient[0] == \".\":\n",
    "        continue\n",
    "    com = np.zeros((44,44))\n",
    "    centroid_df = pd.read_csv(centroid_path + patient)\n",
    "    biom_df = pd.read_csv(binary_infopath + patient)\n",
    "    vor = create_voronoi(centroid_df)\n",
    "    edges = vor.ridge_points\n",
    "    # Iterate through the ridge points because they define a certain combination\n",
    "    for edge in edges:\n",
    "        # Find the indices of the cells that the edge separates\n",
    "        first_centroid = int(edge[0])\n",
    "        second_centroid = int(edge[1])\n",
    "        \n",
    "        # Find their expression\n",
    "        first_cell = biom_df.loc[[first_centroid]]\n",
    "        second_cell = biom_df.loc[[second_centroid]]\n",
    "    \n",
    "        first_pos = []\n",
    "        second_pos = []\n",
    "        for column in first_cell.columns[3:]:\n",
    "            if first_cell[column].values[0] == 1:\n",
    "                first_pos.append(int(column))\n",
    "            if second_cell[column].values[0] == 1:\n",
    "                second_pos.append(int(column))\n",
    "        \n",
    "        #Cartesian product of the proteins that each cell in the adjacency are positive for\n",
    "        combinations = product(first_pos, second_pos)\n",
    "        for comb in combinations:\n",
    "            first_marker = comb[0]\n",
    "            second_marker = comb[1]\n",
    "            com[first_marker, second_marker] += 1\n",
    "            if (first_marker != second_marker):\n",
    "                com[second_marker, first_marker] += 1\n",
    "    com_df = pd.DataFrame(com, columns=biom_columns)\n",
    "    com_df.set_index(pd.Index(biom_columns), inplace=True)\n",
    "    com_df.to_csv(\"intermediate_data/created_interaction_matrices/\" + patient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Protein Interactions Analysis <a name=\"functional\"></a>\n",
    "\n",
    "**Purpose**: determine whether interactions involving immunoregulatory proteins are predictors of recurrence and survival.  \n",
    "**Output**: two KM curves with log-rank test p-value in results/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "import seaborn\n",
    "\n",
    "#Path to the clinical data about recurrence. \n",
    "clinical_path = \"rawdata/clinical_data.csv\"\n",
    "clinical_df = pd.read_csv(clinical_path, index_col=[\"ID\"])\n",
    "\n",
    "#Path to interaction matrices\n",
    "matrices_path = \"intermediate_data/interaction_matrices/\"\n",
    "\n",
    "# Out of all proteins, only include the functional proteins\n",
    "markers_to_include = [2,6,14,16,22,23,24,25,26,27,28,29,30,31,35,37,38,39]\n",
    "\n",
    "columns = []\n",
    "feature_list = []\n",
    "\n",
    "#Iterate over all of the co-occurrence matrices in this certain radius. \n",
    "for patient_index in range(len(os.listdir(matrices_path))):\n",
    "    patient_glcm = os.listdir(matrices_path)[patient_index]\n",
    "\n",
    "    #Skip over the pesky .DS_Store file that shows up in Mac file systems.\n",
    "    if (patient_glcm[0] == \".\"):\n",
    "        continue\n",
    "\n",
    "    #Find the internal_ID of the current patient. \n",
    "    identifier = patient_glcm.split(\".\")[0]\n",
    "\n",
    "    #Read the co-occurrence matrix of the current patient and cast to a numpy array excluding the first col.\n",
    "    current_patient_glcm = pd.read_csv(os.path.join(matrices_path, patient_glcm), index_col=\"Unnamed: 0\")\n",
    "    current_patient_glcm.set_index(current_patient_glcm.columns, inplace=True)\n",
    "\n",
    "    #feature_name = current_patient_glcm.columns[chosen_feature]\n",
    "    np_glcm = current_patient_glcm.to_numpy()    \n",
    "\n",
    "    patient_features = []\n",
    "\n",
    "    #Flatten the co-occurrence matrix into a feature vector.\n",
    "    #Cannot simply run np.flatten because the matrix is symmetrical - I only want the top-right triangle\n",
    "    #But, have to include the diagonal as well.\n",
    "    for row in range(np_glcm.shape[0]):\n",
    "        for column in range(row, np_glcm.shape[1]):\n",
    "            if row in markers_to_include and column in markers_to_include:\n",
    "                if patient_index == 0:\n",
    "                    feature_name = current_patient_glcm.index[row] + \" \" + current_patient_glcm.columns[column]\n",
    "                    columns.append(feature_name)\n",
    "                patient_features.append(np_glcm[row][column])\n",
    "\n",
    "    #Find the recurrence outcome of this current patient and how long it took for them to recur\n",
    "    #Add these values to the feature list for eventual use in the Kaplan-Meier plot.\n",
    "    try:\n",
    "        patient_features.append(clinical_df.at[int(identifier), \"Recurrence\"])\n",
    "    except KeyError:\n",
    "        continue\n",
    "    patient_features.append(clinical_df.at[int(identifier), \"Recurrence_time\"])\n",
    "    patient_features.append(int(identifier))\n",
    "    feature_list.append(patient_features)\n",
    "\n",
    "#Determine the names of the columns in the DataFrame for easier future access.\n",
    "columns.append(\"Recurrence\")\n",
    "columns.append(\"Recurrence_time\")\n",
    "columns.append(\"ID\")\n",
    "\n",
    "#Create a dataframe using the features, the recurrence events, and the time taken to recur.\n",
    "features_df = pd.DataFrame(feature_list, columns=columns)\n",
    "\n",
    "#Obtain a versino of this dataframe with only the features.\n",
    "data_only = features_df.drop(columns=[\"Recurrence_time\", \"Recurrence\", \"ID\"])\n",
    "data_only[\"Duplicate\"] = data_only[feature_name]\n",
    "data_only.set_index(features_df[\"ID\"], inplace=True)\n",
    "\n",
    "#Create the dendrogram.\n",
    "clustergram = seaborn.clustermap(data_only, method=\"complete\",\n",
    "                            metric=\"braycurtis\", standard_scale = 1, cmap=\"viridis\", figsize=(10,8), cbar_pos=None)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "#Number of clusters to take from the dendrogram\n",
    "k = 2\n",
    "\n",
    "#Use scipy fcluster to find clusters from the clustered dendrograms.\n",
    "clusters = list(fcluster(clustergram.dendrogram_row.linkage, k, criterion='maxclust'))\n",
    "\n",
    "unique_clusters = len(np.unique(np.array(clusters)))\n",
    "\n",
    "if (unique_clusters < 2):\n",
    "    print (\"Not able to find more than one cluster. \") #If there was no clustering, this method failed.\n",
    "\n",
    "#Create a new column in the DataFrame that includes what cluster each patient falls into\n",
    "features_df[\"clust\"] = clusters\n",
    "\n",
    "first_cluster_count = clusters.count(1)\n",
    "second_cluster_count = clusters.count(2)\n",
    "\n",
    "#Define the KaplanMeierFitter\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "#T = Time, E=Event. These are the two parameters that go into Kaplan-Meier curves. \n",
    "T = features_df[\"Recurrence_time\"]\n",
    "E = features_df[\"Recurrence\"]\n",
    "\n",
    "group1 = (features_df[\"clust\"] == 1)\n",
    "group2 = (features_df[\"clust\"] == 2)\n",
    "\n",
    "T1 = T[group1]\n",
    "E1 = E[group1]\n",
    "T2 = T[group2]\n",
    "E2 = E[group2]\n",
    "\n",
    "color_clust1 = \"#F39B7FFF\"\n",
    "color_clust2 = \"#4DBBD5FF\"\n",
    "\n",
    "# Just for visualization purposes, make the worse-outcome cluster orange and the other blue\n",
    "if E1.mean() < E2.mean():\n",
    "    T1 = T[group2]\n",
    "    E1 = E[group2]\n",
    "    T2 = T[group1]\n",
    "    E2 = E[group1]\n",
    "\n",
    "    features_df[\"clust\"][group1] = 2\n",
    "    features_df[\"clust\"][group2] = 1\n",
    "\n",
    "first_cluster_count = len(T1.index)\n",
    "second_cluster_count = len(T2.index)\n",
    "\n",
    "results_first = logrank_test(T1,T2, event_observed_A=E1, event_observed_B=E2)\n",
    "\n",
    "p1 = round(results_first.p_value, 4)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "kmf.fit(T1, E1, label='Cluster 1: n=' + str(first_cluster_count))\n",
    "ax = kmf.plot(ci_show=False, show_censors=True, color=color_clust1, lw=5)\n",
    "\n",
    "kmf.fit(T2, E2, label='Cluster 2: n=' + str(second_cluster_count))\n",
    "ax = kmf.plot(ax=ax, ci_show=False, show_censors=True, color=color_clust2, lw=5)\n",
    "plt.annotate(\"Log-rank p: \" + str(p1), xy=(0.6, 0.15), xycoords=\"figure fraction\", fontsize=25,\n",
    "        bbox=dict(facecolor='none', edgecolor='black', alpha=0.3, boxstyle=\"Round, pad=0.5, rounding_size=0.2\"))\n",
    "plt.legend(fontsize=25)\n",
    "plt.xlabel(\"Time (days)\", fontsize=25)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Proportion Alive\", fontsize=25)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim(0,1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"results/functional_interactions_km_recurrence.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the clinical data. \n",
    "clinical_path = \"rawdata/clinical_data.csv\"\n",
    "clinical_df = pd.read_csv(clinical_path, index_col=[\"ID\"])\n",
    "\n",
    "#Path to interaction matrices\n",
    "matrices_path = \"intermediate_data/interaction_matrices/\"\n",
    "\n",
    "# Out of all proteins, only include the functional proteins\n",
    "markers_to_include = [2,6,14,16,22,23,24,25,26,27,28,29,30,31,35,37,38,39]\n",
    "\n",
    "columns = []\n",
    "feature_list = []\n",
    "\n",
    "#Iterate over all of the co-occurrence matrices in this certain radius. \n",
    "for patient_index in range(len(os.listdir(matrices_path))):\n",
    "    patient_glcm = os.listdir(matrices_path)[patient_index]\n",
    "\n",
    "    #Skip over the pesky .DS_Store file that shows up in Mac file systems.\n",
    "    if (patient_glcm[0] == \".\"):\n",
    "        continue\n",
    "\n",
    "    #Find the internal_ID of the current patient. \n",
    "    identifier = patient_glcm.split(\".\")[0]\n",
    "\n",
    "    #Read the co-occurrence matrix of the current patient and cast to a numpy array excluding the first col.\n",
    "    current_patient_glcm = pd.read_csv(os.path.join(matrices_path, patient_glcm), index_col=\"Unnamed: 0\")\n",
    "    current_patient_glcm.set_index(current_patient_glcm.columns, inplace=True)\n",
    "\n",
    "    #feature_name = current_patient_glcm.columns[chosen_feature]\n",
    "    np_glcm = current_patient_glcm.to_numpy()    \n",
    "\n",
    "    patient_features = []\n",
    "\n",
    "    #Flatten the co-occurrence matrix into a feature vector.\n",
    "    #Cannot simply run np.flatten because the matrix is symmetrical - I only want the top-right triangle\n",
    "    #But, have to include the diagonal as well.\n",
    "    for row in range(np_glcm.shape[0]):\n",
    "        for column in range(row, np_glcm.shape[1]):\n",
    "            if row in markers_to_include and column in markers_to_include:\n",
    "                if patient_index == 0:\n",
    "                    feature_name = current_patient_glcm.index[row] + \" \" + current_patient_glcm.columns[column]\n",
    "                    columns.append(feature_name)\n",
    "                patient_features.append(np_glcm[row][column])\n",
    "\n",
    "    #Find the survival outcome of this current patient and how long it took for them to recur\n",
    "    #Add these values to the feature list for eventual use in the Kaplan-Meier plot.\n",
    "    try:\n",
    "        patient_features.append(clinical_df.at[int(identifier), \"Survival\"])\n",
    "    except KeyError:\n",
    "        continue\n",
    "    patient_features.append(clinical_df.at[int(identifier), \"Survival_time\"])\n",
    "    patient_features.append(int(identifier))\n",
    "    feature_list.append(patient_features)\n",
    "\n",
    "#Determine the names of the columns in the DataFrame for easier future access.\n",
    "columns.append(\"Survival\")\n",
    "columns.append(\"Survival_time\")\n",
    "columns.append(\"ID\")\n",
    "\n",
    "#Create a dataframe using the features, the survival events, and the time taken to recur.\n",
    "\n",
    "features_df = pd.DataFrame(feature_list, columns=columns)\n",
    "\n",
    "#Obtain a versino of this dataframe with only the features.\n",
    "data_only = features_df.drop(columns=[\"Survival\", \"Survival_time\", \"ID\"])\n",
    "data_only[\"Duplicate\"] = data_only[feature_name]\n",
    "data_only.set_index(features_df[\"ID\"], inplace=True)\n",
    "\n",
    "#Create the dendrogram.\n",
    "clustergram = seaborn.clustermap(data_only, method=\"complete\",\n",
    "                            metric=\"braycurtis\", standard_scale = 1, cmap=\"viridis\", figsize=(10,8), cbar_pos=None)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "#Number of clusters to take from the dendrogram\n",
    "k = 2\n",
    "\n",
    "#Use scipy fcluster to find clusters from the clustered dendrograms.\n",
    "clusters = list(fcluster(clustergram.dendrogram_row.linkage, k, criterion='maxclust'))\n",
    "\n",
    "unique_clusters = len(np.unique(np.array(clusters)))\n",
    "\n",
    "if (unique_clusters < 2):\n",
    "    print (\"Not able to find more than one cluster. \") #If there was no clustering, this method failed.\n",
    "\n",
    "#Create a new column in the DataFrame that includes what cluster each patient falls into\n",
    "features_df[\"clust\"] = clusters\n",
    "\n",
    "first_cluster_count = clusters.count(1)\n",
    "second_cluster_count = clusters.count(2)\n",
    "\n",
    "#Define the KaplanMeierFitter\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "#T = Time, E=Event. These are the two parameters that go into Kaplan-Meier curves. \n",
    "T = features_df[\"Survival_time\"]\n",
    "E = features_df[\"Survival\"]\n",
    "\n",
    "group1 = (features_df[\"clust\"] == 1)\n",
    "group2 = (features_df[\"clust\"] == 2)\n",
    "\n",
    "T1 = T[group1]\n",
    "E1 = E[group1]\n",
    "T2 = T[group2]\n",
    "E2 = E[group2]\n",
    "\n",
    "color_clust1 = \"#F39B7FFF\"\n",
    "color_clust2 = \"#4DBBD5FF\"\n",
    "\n",
    "# Just for visualization purposes, make the worse-outcome cluster orange and the other blue\n",
    "if E1.mean() < E2.mean():\n",
    "    T1 = T[group2]\n",
    "    E1 = E[group2]\n",
    "    T2 = T[group1]\n",
    "    E2 = E[group1]\n",
    "\n",
    "features_df[\"clust\"][group1] = 2\n",
    "features_df[\"clust\"][group2] = 1\n",
    "\n",
    "first_cluster_count = len(T1.index)\n",
    "second_cluster_count = len(T2.index)\n",
    "\n",
    "results_first = logrank_test(T1,T2, event_observed_A=E1, event_observed_B=E2)\n",
    "\n",
    "p1 = round(results_first.p_value, 4)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "kmf.fit(T1, E1, label='Cluster 1: n=' + str(first_cluster_count))\n",
    "ax = kmf.plot(ci_show=False, show_censors=True, color=color_clust1, lw=5)\n",
    "\n",
    "kmf.fit(T2, E2, label='Cluster 2: n=' + str(second_cluster_count))\n",
    "ax = kmf.plot(ax=ax, ci_show=False, show_censors=True, color=color_clust2, lw=5)\n",
    "plt.annotate(\"Log-rank p: \" + str(p1), xy=(0.6, 0.15), xycoords=\"figure fraction\", fontsize=25,\n",
    "        bbox=dict(facecolor='none', edgecolor='black', alpha=0.3, boxstyle=\"Round, pad=0.5, rounding_size=0.2\"))\n",
    "plt.legend(fontsize=25)\n",
    "plt.xlabel(\"Time (days)\", fontsize=25)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Proportion Alive\", fontsize=25)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim(0,1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"results/functional_interactions_km_survival.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Immunoregulatory Protein Interactions Analysis <a name=\"immuno\"></a>\n",
    "\n",
    "**Purpose**: determine whether interactions involving immunoregulatory proteins are predictors of recurrence and survival.  \n",
    "**Output**: two KM curves with log-rank test p-value in results/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "import seaborn\n",
    "\n",
    "\n",
    "#Path to the clinical data. \n",
    "clinical_path = \"rawdata/clinical_data.csv\"\n",
    "clinical_df = pd.read_csv(clinical_path, index_col=[\"ID\"])\n",
    "\n",
    "#Path to interaction matrices\n",
    "matrices_path = \"intermediate_data/interaction_matrices/\"\n",
    "\n",
    "# Out of all proteins, only include the functional proteins\n",
    "markers_to_include = [27, 31, 37, 38]\n",
    "\n",
    "columns = []\n",
    "feature_list = []\n",
    "\n",
    "#Iterate over all of the co-occurrence matrices in this certain radius. \n",
    "for patient_index in range(len(os.listdir(matrices_path))):\n",
    "    patient_glcm = os.listdir(matrices_path)[patient_index]\n",
    "\n",
    "    #Skip over the pesky .DS_Store file that shows up in Mac file systems.\n",
    "    if (patient_glcm[0] == \".\"):\n",
    "        continue\n",
    "\n",
    "    #Find the internal_ID of the current patient. \n",
    "    identifier = patient_glcm.split(\".\")[0]\n",
    "\n",
    "    #Read the co-occurrence matrix of the current patient and cast to a numpy array excluding the first col.\n",
    "    current_patient_glcm = pd.read_csv(os.path.join(matrices_path, patient_glcm), index_col=\"Unnamed: 0\")\n",
    "    current_patient_glcm.set_index(current_patient_glcm.columns, inplace=True)\n",
    "\n",
    "    #feature_name = current_patient_glcm.columns[chosen_feature]\n",
    "    np_glcm = current_patient_glcm.to_numpy()    \n",
    "\n",
    "    patient_features = []\n",
    "\n",
    "    #Flatten the co-occurrence matrix into a feature vector.\n",
    "    #Cannot simply run np.flatten because the matrix is symmetrical - I only want the top-right triangle\n",
    "    #But, have to include the diagonal as well.\n",
    "    for row in range(np_glcm.shape[0]):\n",
    "        for column in range(row, np_glcm.shape[1]):\n",
    "            if row in markers_to_include and column in markers_to_include:\n",
    "                if patient_index == 0:\n",
    "                    feature_name = current_patient_glcm.index[row] + \" \" + current_patient_glcm.columns[column]\n",
    "                    columns.append(feature_name)\n",
    "                patient_features.append(np_glcm[row][column])\n",
    "\n",
    "    #Find the recurrence outcome of this current patient and how long it took for them to recur\n",
    "    #Add these values to the feature list for eventual use in the Kaplan-Meier plot.\n",
    "    try:\n",
    "        patient_features.append(clinical_df.at[int(identifier), \"Recurrence\"])\n",
    "    except KeyError:\n",
    "        continue\n",
    "    patient_features.append(clinical_df.at[int(identifier), \"Recurrence_time\"])\n",
    "    patient_features.append(int(identifier))\n",
    "    feature_list.append(patient_features)\n",
    "\n",
    "#Determine the names of the columns in the DataFrame for easier future access.\n",
    "columns.append(\"Recurrence\")\n",
    "columns.append(\"Recurrence_time\")\n",
    "columns.append(\"ID\")\n",
    "\n",
    "#Create a dataframe using the features, the recurrence events, and the time taken to recur.\n",
    "features_df = pd.DataFrame(feature_list, columns=columns)\n",
    "\n",
    "#Obtain a versino of this dataframe with only the features.\n",
    "data_only = features_df.drop(columns=[\"Recurrence_time\", \"Recurrence\", \"ID\"])\n",
    "data_only.set_index(features_df[\"ID\"], inplace=True)\n",
    "\n",
    "#Create the dendrogram.\n",
    "clustergram = seaborn.clustermap(data_only, method=\"weighted\",\n",
    "                            metric=\"canberra\", standard_scale = 1, cmap=\"viridis\", figsize=(10,8), cbar_pos=None)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "#Number of clusters to take from the dendrogram\n",
    "k = 2\n",
    "\n",
    "#Use scipy fcluster to find clusters from the clustered dendrograms.\n",
    "clusters = list(fcluster(clustergram.dendrogram_row.linkage, k, criterion='maxclust'))\n",
    "\n",
    "unique_clusters = len(np.unique(np.array(clusters)))\n",
    "\n",
    "if (unique_clusters < 2):\n",
    "    print (\"Not able to find more than one cluster. \") #If there was no clustering, this method failed.\n",
    "\n",
    "#Create a new column in the DataFrame that includes what cluster each patient falls into\n",
    "features_df[\"clust\"] = clusters\n",
    "\n",
    "first_cluster_count = clusters.count(1)\n",
    "second_cluster_count = clusters.count(2)\n",
    "\n",
    "#Define the KaplanMeierFitter\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "#T = Time, E=Event. These are the two parameters that go into Kaplan-Meier curves. \n",
    "T = features_df[\"Recurrence_time\"]\n",
    "E = features_df[\"Recurrence\"]\n",
    "\n",
    "group1 = (features_df[\"clust\"] == 1)\n",
    "group2 = (features_df[\"clust\"] == 2)\n",
    "\n",
    "T1 = T[group1]\n",
    "E1 = E[group1]\n",
    "T2 = T[group2]\n",
    "E2 = E[group2]\n",
    "\n",
    "color_clust1 = \"#F39B7FFF\"\n",
    "color_clust2 = \"#4DBBD5FF\"\n",
    "\n",
    "# Just for visualization purposes, make the worse-outcome cluster orange and the other blue\n",
    "if E1.mean() < E2.mean():\n",
    "    T1 = T[group2]\n",
    "    E1 = E[group2]\n",
    "    T2 = T[group1]\n",
    "    E2 = E[group1]\n",
    "\n",
    "    features_df[\"clust\"][group1] = 2\n",
    "    features_df[\"clust\"][group2] = 1\n",
    "\n",
    "first_cluster_count = len(T1.index)\n",
    "second_cluster_count = len(T2.index)\n",
    "\n",
    "results_first = logrank_test(T1,T2, event_observed_A=E1, event_observed_B=E2)\n",
    "\n",
    "p1 = round(results_first.p_value, 4)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "kmf.fit(T1, E1, label='Cluster 1: n=' + str(first_cluster_count))\n",
    "ax = kmf.plot(ci_show=False, show_censors=True, color=color_clust1, lw=5)\n",
    "\n",
    "kmf.fit(T2, E2, label='Cluster 2: n=' + str(second_cluster_count))\n",
    "ax = kmf.plot(ax=ax, ci_show=False, show_censors=True, color=color_clust2, lw=5)\n",
    "plt.annotate(\"Log-rank p: \" + str(p1), xy=(0.6, 0.15), xycoords=\"figure fraction\", fontsize=25,\n",
    "        bbox=dict(facecolor='none', edgecolor='black', alpha=0.3, boxstyle=\"Round, pad=0.5, rounding_size=0.2\"))\n",
    "plt.legend(fontsize=25)\n",
    "plt.xlabel(\"Time (days)\", fontsize=25)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Proportion Alive\", fontsize=25)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim(0,1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"results/immunoregulatory_interactions_km_recurrence.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the clinical data. \n",
    "clinical_path = \"rawdata/clinical_data.csv\"\n",
    "clinical_df = pd.read_csv(clinical_path, index_col=[\"ID\"])\n",
    "\n",
    "#Path to interaction matrices\n",
    "matrices_path = \"intermediate_data/interaction_matrices/\"\n",
    "\n",
    "# Out of all proteins, only include the functional proteins\n",
    "markers_to_include = [27, 31, 37, 38]\n",
    "\n",
    "columns = []\n",
    "feature_list = []\n",
    "\n",
    "#Iterate over all of the co-occurrence matrices in this certain radius. \n",
    "for patient_index in range(len(os.listdir(matrices_path))):\n",
    "    patient_glcm = os.listdir(matrices_path)[patient_index]\n",
    "\n",
    "    #Skip over the pesky .DS_Store file that shows up in Mac file systems.\n",
    "    if (patient_glcm[0] == \".\"):\n",
    "        continue\n",
    "\n",
    "    #Find the internal_ID of the current patient. \n",
    "    identifier = patient_glcm.split(\".\")[0]\n",
    "\n",
    "    #Read the co-occurrence matrix of the current patient and cast to a numpy array excluding the first col.\n",
    "    current_patient_glcm = pd.read_csv(os.path.join(matrices_path, patient_glcm), index_col=\"Unnamed: 0\")\n",
    "    current_patient_glcm.set_index(current_patient_glcm.columns, inplace=True)\n",
    "\n",
    "    #feature_name = current_patient_glcm.columns[chosen_feature]\n",
    "    np_glcm = current_patient_glcm.to_numpy()    \n",
    "\n",
    "    patient_features = []\n",
    "\n",
    "    #Flatten the co-occurrence matrix into a feature vector.\n",
    "    #Cannot simply run np.flatten because the matrix is symmetrical - I only want the top-right triangle\n",
    "    #But, have to include the diagonal as well.\n",
    "    for row in range(np_glcm.shape[0]):\n",
    "        for column in range(row, np_glcm.shape[1]):\n",
    "            if row in markers_to_include and column in markers_to_include:\n",
    "                if patient_index == 0:\n",
    "                    feature_name = current_patient_glcm.index[row] + \" \" + current_patient_glcm.columns[column]\n",
    "                    columns.append(feature_name)\n",
    "                patient_features.append(np_glcm[row][column])\n",
    "\n",
    "    #Find the survival outcome of this current patient and how long it took for them to recur\n",
    "    #Add these values to the feature list for eventual use in the Kaplan-Meier plot.\n",
    "    try:\n",
    "        patient_features.append(clinical_df.at[int(identifier), \"Survival\"])\n",
    "    except KeyError:\n",
    "        continue\n",
    "    patient_features.append(clinical_df.at[int(identifier), \"Survival_time\"])\n",
    "    patient_features.append(int(identifier))\n",
    "    feature_list.append(patient_features)\n",
    "\n",
    "#Determine the names of the columns in the DataFrame for easier future access.\n",
    "columns.append(\"Survival\")\n",
    "columns.append(\"Survival_time\")\n",
    "columns.append(\"ID\")\n",
    "\n",
    "#Create a dataframe using the features, the survival events, and the time taken to recur.\n",
    "\n",
    "features_df = pd.DataFrame(feature_list, columns=columns)\n",
    "\n",
    "#Obtain a versino of this dataframe with only the features.\n",
    "data_only = features_df.drop(columns=[\"Survival\", \"Survival_time\", \"ID\"])\n",
    "data_only.set_index(features_df[\"ID\"], inplace=True)\n",
    "\n",
    "#Create the dendrogram.\n",
    "clustergram = seaborn.clustermap(data_only, method=\"weighted\",\n",
    "                            metric=\"canberra\", standard_scale = 1, cmap=\"viridis\", figsize=(10,8), cbar_pos=None)\n",
    "\n",
    "plt.close()\n",
    "\n",
    "#Number of clusters to take from the dendrogram\n",
    "k = 2\n",
    "\n",
    "#Use scipy fcluster to find clusters from the clustered dendrograms.\n",
    "clusters = list(fcluster(clustergram.dendrogram_row.linkage, k, criterion='maxclust'))\n",
    "\n",
    "unique_clusters = len(np.unique(np.array(clusters)))\n",
    "\n",
    "if (unique_clusters < 2):\n",
    "    print (\"Not able to find more than one cluster. \") #If there was no clustering, this method failed.\n",
    "\n",
    "#Create a new column in the DataFrame that includes what cluster each patient falls into\n",
    "features_df[\"clust\"] = clusters\n",
    "\n",
    "first_cluster_count = clusters.count(1)\n",
    "second_cluster_count = clusters.count(2)\n",
    "\n",
    "#Define the KaplanMeierFitter\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "#T = Time, E=Event. These are the two parameters that go into Kaplan-Meier curves. \n",
    "T = features_df[\"Survival_time\"]\n",
    "E = features_df[\"Survival\"]\n",
    "\n",
    "group1 = (features_df[\"clust\"] == 1)\n",
    "group2 = (features_df[\"clust\"] == 2)\n",
    "\n",
    "T1 = T[group1]\n",
    "E1 = E[group1]\n",
    "T2 = T[group2]\n",
    "E2 = E[group2]\n",
    "\n",
    "color_clust1 = \"#F39B7FFF\"\n",
    "color_clust2 = \"#4DBBD5FF\"\n",
    "\n",
    "# Just for visualization purposes, make the worse-outcome cluster orange and the other blue\n",
    "if E1.mean() < E2.mean():\n",
    "    T1 = T[group2]\n",
    "    E1 = E[group2]\n",
    "    T2 = T[group1]\n",
    "    E2 = E[group1]\n",
    "\n",
    "features_df[\"clust\"][group1] = 2\n",
    "features_df[\"clust\"][group2] = 1\n",
    "\n",
    "first_cluster_count = len(T1.index)\n",
    "second_cluster_count = len(T2.index)\n",
    "\n",
    "results_first = logrank_test(T1,T2, event_observed_A=E1, event_observed_B=E2)\n",
    "\n",
    "p1 = round(results_first.p_value, 4)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "kmf.fit(T1, E1, label='Cluster 1: n=' + str(first_cluster_count))\n",
    "ax = kmf.plot(ci_show=False, show_censors=True, color=color_clust1, lw=5)\n",
    "\n",
    "kmf.fit(T2, E2, label='Cluster 2: n=' + str(second_cluster_count))\n",
    "ax = kmf.plot(ax=ax, ci_show=False, show_censors=True, color=color_clust2, lw=5)\n",
    "plt.annotate(\"Log-rank p: \" + str(p1), xy=(0.6, 0.15), xycoords=\"figure fraction\", fontsize=25,\n",
    "        bbox=dict(facecolor='none', edgecolor='black', alpha=0.3, boxstyle=\"Round, pad=0.5, rounding_size=0.2\"))\n",
    "plt.legend(fontsize=25)\n",
    "plt.xlabel(\"Time (days)\", fontsize=25)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Proportion Alive\", fontsize=25)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylim(0,1)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"results/immunoregulatory_interactions_km_survival.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Analysis 1: Cox Regression <a name=\"multi_cox\"></a>\n",
    "**Purpose**: perform multivariate Cox regression.  \n",
    "**Demo Note**: the reader should change the type of cluster to be examined based on the options given in the comments.  \n",
    "**Output**: a model summary printed to output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lifelines\n",
    "import seaborn as sns\n",
    "\n",
    "CSV_PATH = \"intermediate_data/covariate_rsf_data.csv\" #put path to csv summarizing cluster features, clinical variables, architecutre distinction, and clinical outcome\n",
    "\n",
    "#Read csv summarizing cluster features, clinical variables, morphology distinction, and clinical outcome\n",
    "df = pd.read_csv(CSV_PATH, index_col=\"ID\")\n",
    "\n",
    "#define which cluster feature to examine\n",
    "#options: [coexpression_cluster, functional_proteins_cluster, immunoregulatory_protein_cluster]\n",
    "#can either examine each one-at-a-time or iterate through them\n",
    "cluster_choice = \"coexpression_cluster\" \n",
    "\n",
    "#manipulate the architecture distinction the variable which is originally dtype: str into a quantitative categorical variable\n",
    "df[\"Architecture\"] = df[\"Architecture\"].astype(\"category\").cat.codes\n",
    "\n",
    "#Create two separate feature matrices for recurrence and survival\n",
    "recurrence_df = df.drop(columns=[\"Survival\", \"Survival_time\"])[[cluster_choice, \"grade\", \"age\", \"Architecture\", \"Recurrence\", \"Recurrence_time\"]]\n",
    "survival_df = df.drop(columns=[\"Recurrence\", \"Recurrence_time\"])[[cluster_choice, \"grade\", \"age\", \"Architecture\", \"Survival\", \"Survival_time\"]]\n",
    "\n",
    "#Define and fit Cox PH Fitter for recurrence\n",
    "recurrence_cph = lifelines.CoxPHFitter()\n",
    "recurrence_cph.fit(recurrence_df, duration_col='Recurrence_time', event_col='Recurrence')\n",
    "recurrence_summary = recurrence_cph.print_summary()\n",
    "\n",
    "#Define and fit Cox PH Fitter for survival\n",
    "survival_cph = lifelines.CoxPHFitter()\n",
    "survival_cph.fit(survival_df, duration_col='Survival_time', event_col='Survival')\n",
    "survival_summary = survival_cph.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Analysis 2: Random Survival Forest <a name=\"rsf\"></a>\n",
    "**Purpose**: build a random survival forest to evaluate importance and measure model accuracy.  \n",
    "**Output**: an importance plot in results/ and a concordance index printed to output.  \n",
    "**Estimated time**: 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from pysurvival.models import survival_forest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pysurvival.models import survival_forest\n",
    "from pysurvival.utils.metrics import concordance_index\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "#Calculating Shapley values\n",
    "\n",
    "#Provide the path to the csv with the multivariate data\n",
    "CSV_PATH = \"intermediate_data/covariate_rsf_data.csv\"\n",
    "\n",
    "#Import CSV into a dataframe\n",
    "data = pd.read_csv(CSV_PATH, index_col=\"ID\")\n",
    "\n",
    "#Manipulate the architecture feature from a string into a quantitative categorical variable\n",
    "data[\"Architecture\"] = data[\"Architecture\"].astype(\"category\").cat.codes\n",
    "\n",
    "#Define the random survival forest model\n",
    "rf = survival_forest.RandomSurvivalForestModel(num_trees = 200)\n",
    "\n",
    "#Define predictors and response for inputting into the model\n",
    "X = data[[\"functional_proteins_cluster\", \"immunoregulatory_protein_cluster\", \"coexpression_cluster\", \"age\", \"grade\", \"Architecture\"]]\n",
    "T = data[\"Survival_time\"]\n",
    "E = data[\"Survival\"]\n",
    "\n",
    "#Fit the RSF\n",
    "fitted = rf.fit(X=X, T=T, E=E, max_depth=5)\n",
    "\n",
    "#Train the Shap KernelExplainer and calculate Shapley values\n",
    "explainer = shap.KernelExplainer(fitted.predict_risk, data=X)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "#Plot the Shapley values as a bar plot\n",
    "fig = shap.summary_plot(shap_values, features=X, plot_type='bar')\n",
    "\n",
    "plt.savefig(\"results/importance_plot.png\", dpi=300)\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "#Evaluating model performance\n",
    "\n",
    "#Define a number of iterations to fit the model on different seeds\n",
    "iterations = 1000\n",
    "\n",
    "importance_array = np.zeros((6))\n",
    "concordances = []\n",
    "\n",
    "\n",
    "for iter in range(iterations):\n",
    "    data_train, data_test = train_test_split(data, test_size=0.2, random_state=iter)\n",
    "    X_train = data_train[[\"functional_proteins_cluster\", \"immunoregulatory_protein_cluster\", \"coexpression_cluster\", \n",
    "              \"age\", \"grade\", \"Architecture\"]]\n",
    "    T_train = data_train[\"Recurrence_time\"]\n",
    "    E_train = data_train[\"Recurrence\"]\n",
    "\n",
    "    X_test = data_test[[\"functional_proteins_cluster\", \"immunoregulatory_protein_cluster\", \"coexpression_cluster\", \n",
    "              \"age\", \"grade\", \"Architecture\"]]\n",
    "    T_test = data_test[\"Recurrence_time\"]\n",
    "    E_test = data_test[\"Recurrence\"]\n",
    "\n",
    "    #Fit the RSF according to the training set\n",
    "    fitted = rf.fit(X=X_train, T=T_train, E=E_train, max_depth=5, seed=iter)\n",
    "    concordance = concordance_index(rf, X, T, E, include_ties = True, additional_results=False)\n",
    "    concordances.append(concordance)\n",
    "\n",
    "mean_concordance = np.mean(np.array(concordances))\n",
    "\n",
    "print (\"Mean concordance index: \" + str(round(mean_concordance, 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
